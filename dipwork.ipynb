{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Дипломная работа\n",
    "\n",
    "Цель работы — разработка и экспериментальная оценка системы быстрого поиска в больших текстовых массивах с реализацией полнотекстового, семантического и гибридного поиска, позволяющей совмещать скорость и качество релевантности.\n",
    "\n",
    "Тестирование и оценка методов полнотекстового, семантического и гибридного поиска проводились на корпусе данных, собранном из новостного агрегатора Лента.ру.\n",
    "\n",
    "Объём корпуса: около 800 000 строк текстовых данных, включающих заголовки и полные тексты новостей.\n",
    "\n",
    "Предобработка текста: токенизация, удаление стоп-слов, лемматизации.\n",
    "\n",
    "Цель эксперимента: оценить эффективность и производительность каждого метода поиска на крупном реальном корпусе текстов.\n",
    "\n",
    "Метрики оценки:\n",
    "\n",
    "Precision — точность найденных релевантных документов,\n",
    "\n",
    "Recall — полнота поиска,\n",
    "\n",
    "F1-score — гармоническое среднее Precision и Recall,\n",
    "\n",
    "Latency — среднее время выполнения запроса\n"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Установка библиотек:\n",
    "# pip install elasticsearch sentence-transformers faiss-cpu pandas tqdm nltk pymorphy3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pymorphy3\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import subprocess\n",
    "\n",
    "# Загрузка необходимых ресурсов NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "#Запуск Elasticsearch\n",
    "bat_file = r\"C:\\Users\\zoira\\PycharmProjects\\scientificProject4\\elasticsearch-8.17.3\\bin\\elasticsearch.bat\"\n",
    "subprocess.Popen([\n",
    "    \"powershell\",\n",
    "    \"Start-Process\", \"cmd\",\n",
    "    f'-ArgumentList \"/c {bat_file}\"',\n",
    "    \"-Verb\", \"RunAs\"\n",
    "])\n",
    "\n",
    "url = \"http://localhost:9200\"\n",
    "username = \"elastic\"\n",
    "password = \"Vje+ivqy8byb0-oU=Emr\"\n",
    "\n",
    "max_tries = 15\n",
    "for i in range(max_tries):\n",
    "    try:\n",
    "        response = requests.get(url, auth=(username, password))\n",
    "        if response.status_code == 200:\n",
    "            print(\"Elasticsearch запущен и доступен.\")\n",
    "            break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass\n",
    "    print(f\"Ожидание запуска Elasticsearch... попытка {i+1}/{max_tries}\")\n",
    "    time.sleep(2)\n",
    "else:\n",
    "    raise RuntimeError(\"Не удалось подключиться к Elasticsearch. Проверьте запуск сервера\")\n",
    "\n",
    "#Загрузка данных\n",
    "csv_file = r'C:\\Users\\zoira\\Downloads\\processed_news.csv'\n",
    "chunks = []\n",
    "chunk_size = 10000\n",
    "print(\"Загрузка CSV файла...\")\n",
    "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "    chunks.append(chunk)\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "print(f\"Загружено строк: {len(df)}\")\n",
    "\n",
    "#Подключение к Elasticsearch\n",
    "es = Elasticsearch(\"http://localhost:9200\", basic_auth=(username, password))\n",
    "if not es.ping():\n",
    "    raise RuntimeError(\"Elasticsearch недоступен после запуска!\")\n",
    "\n",
    "#Предобработка текста\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "punctuation_marks = set(string.punctuation)\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def preprocess(text, stop_words, punctuation_marks, morph):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        if token in punctuation_marks:\n",
    "            continue\n",
    "        p = morph.parse(token)\n",
    "        if not p:\n",
    "            continue\n",
    "        lemma = p[0].normal_form\n",
    "        if lemma in stop_words:\n",
    "            continue\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "df[\"clean_content\"] = df[\"title\"].fillna('') + \" \" + df[\"text\"].fillna('')\n",
    "df[\"clean_tokens\"] = df[\"clean_content\"].apply(lambda x: preprocess(x, stop_words, punctuation_marks, morph))\n",
    "\n",
    "#Подготовка документов для индексации\n",
    "docs = [{\n",
    "    \"id\": str(i),\n",
    "    \"title\": row[\"title\"] if isinstance(row[\"title\"], str) else \"\",\n",
    "    \"text\": row[\"text\"] if isinstance(row[\"text\"], str) else \"\",\n",
    "    \"content\": \" \".join(row[\"clean_tokens\"])\n",
    "} for i, row in df.iterrows()]\n",
    "\n",
    "INDEX = \"news-eval\"\n",
    "\n",
    "#Пересоздание индекса\n",
    "if es.indices.exists(index=INDEX):\n",
    "    es.indices.delete(index=INDEX)\n",
    "es.indices.create(index=INDEX, ignore=400)\n",
    "\n",
    "#Bulk индексация\n",
    "actions = ({\n",
    "    \"_index\": INDEX,\n",
    "    \"_id\": doc[\"id\"],\n",
    "    \"_source\": doc\n",
    "} for doc in docs)\n",
    "helpers.bulk(es, actions)\n",
    "\n",
    "\n",
    "#Семантический поиск через BERT и FAISS\n",
    "model = SentenceTransformer(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "#Генерация эмбеддингов\n",
    "texts = [doc[\"content\"] for doc in docs]\n",
    "embs = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "dimension = embs.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embs)\n",
    "id_map = {i: doc[\"id\"] for i, doc in enumerate(docs)}\n",
    "\n",
    "#Метки релевантности для оценки\n",
    "true_relevance = {\n",
    "    \"образование\": {\"0\", \"1\", \"4\"},\n",
    "    \"экономика\": {\"2\", \"5\"},\n",
    "    \"спорт\": {\"3\", \"6\"}\n",
    "}\n",
    "\n",
    "def evaluate(query, top_k=5):\n",
    "    true_ids = true_relevance.get(query, set())\n",
    "\n",
    "    # Полнотекстовый поиск\n",
    "    t1 = time.time()\n",
    "    es_resp = es.search(index=INDEX, query={\"match\": {\"content\": query}}, size=top_k)\n",
    "    t2 = time.time()\n",
    "    ft_ids = {hit[\"_id\"] for hit in es_resp[\"hits\"][\"hits\"]}\n",
    "\n",
    "    # Семантический поиск\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    t3 = time.time()\n",
    "    _, I = faiss_index.search(q_emb, top_k)\n",
    "    t4 = time.time()\n",
    "    sem_ids = {id_map[i] for i in I[0]}\n",
    "\n",
    "    # Гибридный поиск\n",
    "    hybrid_ids = ft_ids & sem_ids\n",
    "    t5 = time.time()\n",
    "\n",
    "    docs_ids = [doc[\"id\"] for doc in docs]\n",
    "\n",
    "    def compute_metrics(pred_ids, true_ids):\n",
    "        y_true = [doc_id in true_ids for doc_id in docs_ids]\n",
    "        y_pred = [doc_id in pred_ids for doc_id in docs_ids]\n",
    "        return {\n",
    "            \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "            \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "        }\n",
    "\n",
    "    metrics = {\n",
    "        \"Fulltext\": compute_metrics(ft_ids, true_ids),\n",
    "        \"Semantic\": compute_metrics(sem_ids, true_ids),\n",
    "        \"Hybrid\": compute_metrics(hybrid_ids, true_ids)\n",
    "    }\n",
    "\n",
    "    latency = {\n",
    "        \"Fulltext\": (t2 - t1) * 1000,\n",
    "        \"Semantic\": (t4 - t3) * 1000,\n",
    "        \"Hybrid\": (t5 - t3) * 1000\n",
    "    }\n",
    "\n",
    "    return metrics, latency\n",
    "\n",
    "#Пример оценки\n",
    "query = \"образование\"\n",
    "metrics, latency = evaluate(query)\n",
    "\n",
    "print(f\"\\nСравнение методов по запросу: '{query}'\")\n",
    "print(\"Метод       | Precision | Recall | F1   | Latency (ms)\")\n",
    "for method in metrics:\n",
    "    m = metrics[method]\n",
    "    l = latency[method]\n",
    "    print(f\"{method:<12} | {m['precision']:.2f}     | {m['recall']:.2f}   | {m['f1']:.2f} | {l:.1f}\")\n"
   ],
   "id": "fbc121e30a2defb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
